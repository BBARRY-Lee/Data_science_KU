library(ggplot2)
library(dplyr)

PL <- read.csv("Personal Loan.csv")

# 의사결정 나무  -> 범주형 변수
# 회귀 나무 -> 연속형 변수

# riding mower : 잔디깎기

#의사결정 나무 개요
# 부모노드 -> 자식노드 -> 맨 하위 자식노드로 갈수록 순도가 높아짐
#RSS : sumazation(yi - yhat)^2

# 순도가 최대 일 때, 지니계수 0 -> 작아질수록 순도가 높아진다.
# 지니계수, 데비언스 : 정보획득을 크게 하는 방향으로 가지치기를 해야한다.
# 정보획득이 가장 큰 지점에서 분기를 해야함 -> 데이터가 24개가 있다면 23개의 분기가 가능 (1개의 데이터) -> 가로
# -> 다른 변수를 기준으로 또 분기 -> 세로 -> 이로써, 총 46개의 분기를 하게 됨 -> 이 중에서 정보획득 최대값
# -> N(관측치) x m-1 (설명변수-1)
# 하나의 변수로 분기를 했다면, 꼭 그 변수가 아닌 다른 변수로 분기를 하지 않아도 된다. A라는 변수로 분기했다면 B로 분기하는 것이 아니라, 또 A로 분기해도 됨

# Full node : 순도 100%의 재귀적 분기 -> overfitting 문제가 있음

# overfitting : 에러로 간주해야할 것까지, 정답 혹은 법칙으로 간주하는 것
# ERR (오분류율)

### Post pruning (사후적 가지치기 : 백스텝)
# 1. 재귀적 분기을 통한 Full Tree 생성 -> 2. 비용복잡도 측면에서 효율적이도록 가지치기 n번 -> 최종 모형 선정 (early stopping이 생각난다)

### Pre pruning (사전적 가지치기) -> 재귀적 분기를 하다가 멈춘다. (이게 early stopping에 더 가까운듯!)


#정분류율가지고 모형이 얼마나 예측을 잘 했는지 판단한다 -> 하지만 문제가 있다!?